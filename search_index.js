var documenterSearchIndex = {"docs":
[{"location":"overview/#","page":"-","title":"-","text":"WaspNet aims to provide a flexible framework with which to simulate collections of neurons. ","category":"page"},{"location":"overview/#Abstractions-1","page":"-","title":"Abstractions","text":"","category":"section"},{"location":"overview/#","page":"-","title":"-","text":"All WaspNet elements are subtypes of the abstract type WaspnetElement","category":"page"},{"location":"overview/#Neuron-1","page":"-","title":"Neuron","text":"","category":"section"},{"location":"overview/#","page":"-","title":"-","text":"A neuron is a collection of parameters describing itself along with the current state of that neuron, typically a concrete subtype of AbstractNeuron. ","category":"page"},{"location":"overview/#Layer-1","page":"-","title":"Layer","text":"","category":"section"},{"location":"overview/#Network-1","page":"-","title":"Network","text":"","category":"section"},{"location":"reference/#Reference-1","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/#Neurons-1","page":"Reference","title":"Neurons","text":"","category":"section"},{"location":"reference/#LIF-1","page":"Reference","title":"LIF","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"\tWaspNet.LIF","category":"page"},{"location":"reference/#WaspNet.LIF","page":"Reference","title":"WaspNet.LIF","text":"LIF{T<:Number}<:AbstractNeuron\n\nContains the necessary parameters for describing a Leaky Integrate-and-Fire (LIF) neuron as well as the current membrane potential of the neuron.\n\nFields\n\nτ::T: Neuron time constant (ms)\nR::T: Neuronal model resistor (kOhms)\nθ::T: Threshold voltage (mV)\nI::T: Background current injection (mV)\nv0::T: Reset voltage (mV)\nstate::T: Current membrane potential (mV)\n\n\n\n\n\n","category":"type"},{"location":"reference/#","page":"Reference","title":"Reference","text":"Modules = [WaspNet]\nPages = [\"LIF.jl\"]\nOrder = [:type, :function]","category":"page"},{"location":"reference/#WaspNet.update-Tuple{WaspNet.LIF,Any,Any,Any}","page":"Reference","title":"WaspNet.update","text":"update!(neuron::LIF, input_update, dt, t)\n\nEvolve and LIF neuron subject to a membrane potential step of size input_update a time duration dt starting from time t\n\n\n\n\n\n","category":"method"},{"location":"reference/#Izh-1","page":"Reference","title":"Izh","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"\tWaspNet.Izh","category":"page"},{"location":"reference/#WaspNet.Izh","page":"Reference","title":"WaspNet.Izh","text":"struct Izh{T<:Number}<:AbstractNeuronn\n\nContains the vector of paramters [a, b, c, d, I, θ] necessary to simulate an Izhikevich neuron as well as the current state of the neuron.\n\nThe @with_kw macro is used to produce a constructor which accepts keyword arguments for all values. This neuron struct is immutable, therefor we store the state of the neuron in an Array such that its values can change while the parameters remain static. This represents a minimal example for an AbstractNeuron implementation to build it into a Layer.\n\nFields\n\na::T-d::T: Neuron parameters as described at https://www.izhikevich.org/publications/spikes.htm\nI::T: Background current (mA)\nθ::T: Threshold potential (mV)\nv0::T: Reset voltage (mV)\nu0::T: Reset recovery variable value\nstate::T: Vector holding the current (v,u) state of the neuron\noutput::T: Vector holding the current output of the neuron\n\n\n\n\n\n","category":"type"},{"location":"reference/#","page":"Reference","title":"Reference","text":"Modules = [WaspNet]\nPages = [\"Izh.jl\"]\nOrder = [:type, :function]","category":"page"},{"location":"reference/#WaspNet.update-Tuple{WaspNet.Izh,Any,Any,Any}","page":"Reference","title":"WaspNet.update","text":"update(neuron::Izh, input_update, dt, t)\n\nEvolves the given Neuron subject to an input of input_update a time duration dt starting from time t according to the equations defined in the Izhikevich paper https://www.izhikevich.org/publications/spikes.htm\n\nWe use an Euler update for solving the set of differential equations for its computational efficiency and simplicity of implementation.\n\n\n\n\n\n","category":"method"},{"location":"reference/#WaspNet.reset-Tuple{WaspNet.Izh}","page":"Reference","title":"WaspNet.reset","text":"reset(neuron::Izh)\n\nResets the state of the Izhikevich neuron to its initial values given by v0, u0\n\n\n\n\n\n","category":"method"},{"location":"reference/#Functional-Neurons-1","page":"Reference","title":"Functional Neurons","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"\tWaspNet.Functional","category":"page"},{"location":"reference/#WaspNet.Functional","page":"Reference","title":"WaspNet.Functional","text":"struct Functional{T<:Number, F<:Function}<:AbstractNeuron\n\nA neuron type which applies some scalar function to its input and returns that value as both its state and output.\n\nFields\n\nfunc::F: A scalar function to apply to all inputs\nstate::T: The last value computed by this neuron's function\n\n\n\n\n\n","category":"type"},{"location":"reference/#","page":"Reference","title":"Reference","text":"Modules = [WaspNet]\nPages = [\"functional.jl\"]\nOrder = [:type, :function]","category":"page"},{"location":"reference/#Layers-1","page":"Reference","title":"Layers","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"Modules = [WaspNet]\nPages = [\"layer.jl\"]\nOrder = [:type, :function]","category":"page"},{"location":"reference/#WaspNet.Layer","page":"Reference","title":"WaspNet.Layer","text":"Layer{\n    L<:AbstractNeuron, N<:Number, A<:AbstractArray{N,1}, M<:Union{AbstractArray{N,2}, Array{AbstractArray{N,2},1}\n    }<:AbstractLayer\n\nTrack a population of neurons of one AbstractNeuron type, the other Layers those neurons are connected to, and the incoming weights. \n\nFields\n\nneurons::Array{L,1}: an array of neurons for the Layer\nW<:Union{Matrix,AbstractBlockArray}: either a Matrix or BlockArray containing weights for inputs from incoming layers\nconns: either [] or Array{Int,1} indicating which Layers in the Network are connected as inputs to this Layer\ninput::Array{N,1}: a pre-allocated array of zeros for staging inputs to the layer\noutput::Array{N,1}: a pre-allocated array for staging outputs from this layer\n\n\n\n\n\n","category":"type"},{"location":"reference/#WaspNet.Layer","page":"Reference","title":"WaspNet.Layer","text":"Layer(neurons, W[, conns = Array{Int,1}()])\n\nConstructs a Layer with constituent neurons which accept inputs from the Layers denoted by conns (input 1 is the Network input) and either a BlockArray of weights if length(conns) > 1 or a Matrix of weights otherwise.\n\n\n\n\n\n","category":"type"},{"location":"reference/#WaspNet.Layer-Union{Tuple{M}, Tuple{A}, Tuple{N}, Tuple{L}, Tuple{J}, Tuple{Array{L,1},M,Array{J,1},Int64,A,A}} where M<:Union{AbstractArray{N,2}, Array{#s59,1} where #s59<:AbstractArray{N,2}} where A<:AbstractArray{N,1} where N<:Number where L<:AbstractNeuron where J","page":"Reference","title":"WaspNet.Layer","text":"Layer(neurons, W, conns, N_neurons, input, output)\n\nDefault non-parametric constructor for Layers for pre-processing inputs and computing parametric types.\n\n\n\n\n\n","category":"method"},{"location":"reference/#WaspNet.reset!-Tuple{AbstractLayer}","page":"Reference","title":"WaspNet.reset!","text":"reset!(l::AbstractLayer)\n\nReset all of the neurons in l to the state defined by their reset! function.\n\n\n\n\n\n","category":"method"},{"location":"reference/#WaspNet.update!-Union{Tuple{M}, Tuple{A}, Tuple{N}, Tuple{L}, Tuple{Layer{L,N,A,M},Any,Any,Any}} where M<:(AbstractArray{#s55,1} where #s55<:AbstractArray) where A where N where L","page":"Reference","title":"WaspNet.update!","text":"function update!(l::Layer{L,N,A,M}, input, dt, t) where {L,N,A, M<:AbstractArray{T,1}}\n\nEvolve the state of all of the neurons in the Layer a duration dt, starting from time t, subject to a set of inputs from all Network layers in input. \n\nNot all arrays within input are used; we iterate over l.conn to select the appropriate inputs to this Layer, and the corresponding Blocks from l.W are used to calculate the net Layer input.\n\n\n\n\n\n","category":"method"},{"location":"reference/#WaspNet.update!-Union{Tuple{M}, Tuple{A}, Tuple{N}, Tuple{L}, Tuple{Layer{L,N,A,M},Any,Any,Any}} where M<:AbstractArray{N,2} where A where N where L","page":"Reference","title":"WaspNet.update!","text":"function update!(l::Layer, input, dt, t)\n\nEvolve the state of all of the neurons in the Layer a duration dt, starting from time t, subject to a set of inputs from all Network layers in input.\n\nThis (default) method assumes a feed-forward, non-BlockArray representation for l.W\n\nArguments\n\nl::Layer: the Layer to be evolved\ninput: an Array of Arrays of output values from other Layers potentially being input to l\ndt: the time step to evolve the Layer\nt: the time at the start of the current time step\n\n\n\n\n\n","category":"method"},{"location":"reference/#WaspNet.update!-Union{Tuple{M}, Tuple{A}, Tuple{N}, Tuple{L}, Tuple{Layer{L,N,A,M},Any,Any,Any}} where M<:BlockArrays.AbstractBlockArray where A where N where L","page":"Reference","title":"WaspNet.update!","text":"function update!(l::Layer{L,N,A,M}, input, dt, t)\n\nEvolve the state of all of the neurons in the Layer a duration dt, starting from time t, subject to a set of inputs from all Network layers in input. \n\nNot all arrays within input are used; we iterate over l.conn to select the appropriate inputs to this Layer, and the corresponding Blocks from l.W are used to calculate the net Layer input.\n\n\n\n\n\n","category":"method"},{"location":"reference/#WaspNet.get_neuron_count-Tuple{AbstractLayer}","page":"Reference","title":"WaspNet.get_neuron_count","text":"get_neuron_count(l::AbstractLayer)\n\nReturn the number of neurons in the given Layer\n\n\n\n\n\n","category":"method"},{"location":"reference/#WaspNet.get_neuron_outputs-Tuple{AbstractLayer}","page":"Reference","title":"WaspNet.get_neuron_outputs","text":"get_neuron_outputs(l::AbstractLayer)\n\nReturn the current output of l's constituent neurons \n\n\n\n\n\n","category":"method"},{"location":"reference/#WaspNet.get_neuron_states-Tuple{AbstractLayer}","page":"Reference","title":"WaspNet.get_neuron_states","text":"get_neuron_states(l::AbstractLayer)\n\nReturn the current state of l's constituent neurons\n\n\n\n\n\n","category":"method"},{"location":"reference/#Networks-1","page":"Reference","title":"Networks","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"Modules = [WaspNet]\nPages = [\"network.jl\"]\nOrder = [:type, :function]","category":"page"},{"location":"reference/#WaspNet.Network","page":"Reference","title":"WaspNet.Network","text":"mutable struct Network<:AbstractNetwork\n\nContains constituent Layers, orchestrates the movement of signals between Layers, and handles first-layer input.\n\nFields\n\nlayers::Array{AbstractLayer,1}: Array of Layers ordered from 1 to N for N layers\nN_in::Int: Number of input dimensions to the first Layer\nprev_outputs::Vector: Vector of vectors sized to hold the output from each Layer \n\n\n\n\n\n","category":"type"},{"location":"reference/#WaspNet.Network-Tuple{Any,Int64}","page":"Reference","title":"WaspNet.Network","text":"function Network(layers, N_in::Int)\n\nGiven an array of Layers and the dimensionality of the input to the network, make a new Network which is a copy of each Layer with weights converted to BlockArray format.\n\nThe output dimensionality is in \n\n\n\n\n\n","category":"method"},{"location":"reference/#WaspNet.Network-Union{Tuple{Array{L,1}}, Tuple{L}} where L<:AbstractLayer","page":"Reference","title":"WaspNet.Network","text":"function Network(layers::Array{L, 1}) where L <: AbstractLayer\n\nGiven an array of Layers, constructs the Network resulting from connecting the Layers with their specified conns. \n\nThe input dimensionality is inferred from the size of the weight matrices for the first Layer in the layers array.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Simulations-1","page":"Reference","title":"Simulations","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"Modules = [WaspNet]\nPages = [\"simulate.jl\"]\nOrder = [:type, :function]","category":"page"},{"location":"reference/#WaspNet.SimulationResult","page":"Reference","title":"WaspNet.SimulationResult","text":"struct SimulationResult{\n    OT<:AbstractArray{<:Number,2}, ST<:AbstractArray{<:Number, 2}, TT<:AbstractArray{<:Real,1}\n    }<:AbstractSimulation\n\nContains simulation results from simulating a Network for a specific length of time with simulate!\n\nFields\n\noutputs::OT: A Matrix containing the output of all simulated neurons at every time step.\nstates::ST: A Matrix containing the state of all simulated neurons at every time step. If states were not tracked, an Nx0 dimensional Matrix.\ntimes::TT: An Array of times at which the WaspnetElement was sampled.\n\n\n\n\n\n","category":"type"},{"location":"reference/#WaspNet.SimulationResult-Union{Tuple{TT}, Tuple{EL}, Tuple{EL,TT}} where TT<:(AbstractArray{#s59,1} where #s59<:Real) where EL<:WaspnetElement","page":"Reference","title":"WaspNet.SimulationResult","text":"SimulationResult(element::EL, times::TT) where {EL<:WaspnetElement,TT<:AbstractArray{<:Real, 1}}\n\nGiven a WaspnetElement and the times at which to simulate the element, construct the SimulationResult instance to store the results of the simulation. \n\n\n\n\n\n","category":"method"},{"location":"reference/#WaspNet.simulate!","page":"Reference","title":"WaspNet.simulate!","text":"simulate!(element::WaspnetElement, input::Matrix, dt, tf, t0 = 0.; track_state=false, kwargs...)\n\nSimulates the supplied WaspnetElement subject to some pre-sampled input where each column is one time step and returns the relevant SimulationResult instance\n\n\n\n\n\n","category":"function"},{"location":"reference/#WaspNet.simulate!","page":"Reference","title":"WaspNet.simulate!","text":"simulate!(element::WaspnetElement, input::Function, dt, tf, t0 = 0.; track_state=false, kwargs...)\n\nSimulates the supplied WaspnetElement subject to a function of time, input by sampling input at the chosen sample times and returns the relevant SimulationResult instance\n\n\n\n\n\n","category":"function"},{"location":"reference/#WaspNet.sim_update!-Tuple{WaspnetElement,Any,Any,Any}","page":"Reference","title":"WaspNet.sim_update!","text":"function sim_update!(ne::WaspnetElement, input_update, dt, t)\n\nGeneric function for wrapping calls to update! from simulate!\n\n\n\n\n\n","category":"method"},{"location":"reference/#WaspNet.sim_update!-Union{Tuple{N}, Tuple{T}, Tuple{AbstractNeuron,AbstractArray{T,N},Any,Any}} where N where T<:Number","page":"Reference","title":"WaspNet.sim_update!","text":"function sim_update!(neuron::AbstractNeuron, input_update<:AbstractArray{T,N}, dt, t) where {T<:Number, N}\n\nWrapper to ensure that if a 1D array is passed to update a neuron, it is converted to a scalar first\n\n\n\n\n\n","category":"method"},{"location":"reference/#Utilities-1","page":"Reference","title":"Utilities","text":"","category":"section"},{"location":"reference/#General-Utilities-1","page":"Reference","title":"General Utilities","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"Modules = [WaspNet]\nPages = [\"src/utilities/utils.jl\"]\nOrder = [:type, :function]","category":"page"},{"location":"reference/#Pruning-1","page":"Reference","title":"Pruning","text":"","category":"section"},{"location":"reference/#","page":"Reference","title":"Reference","text":"Modules = [WaspNet]\nPages = [\"src/utilities/pruning.jl\"]\nOrder = [:type, :function]","category":"page"},{"location":"reference/#WaspNet.delete_entries-Tuple{Any,Any}","page":"Reference","title":"WaspNet.delete_entries","text":"function delete_entries(W, entries; axis::Int = 1)\n\nGiven an AbstractArray, deletes the specified entries (e.g. rows or columns) along the given axis; used for pruning weight matrices. \n\nAs an example, delete_entries(W, [3,4]; axis = 2) would delete columns 3 and 4 from W and return the modified W.\n\n\n\n\n\n","category":"method"},{"location":"reference/#WaspNet.prune-Tuple{WaspnetElement,Any,Any}","page":"Reference","title":"WaspNet.prune","text":"function prune(el::WaspnetElement, layers, neurons[, l_idx])\n\nGiven an element el along with indices for target Neurons, constructs new Layers and Networks with all references to those neurons removed by deleting rows and columns from the proper weight matrices in each Layer.\n\nlayers should be an array of indices relative to the Network it is being pruned in; neurons should be an array of arrays of indices where the entries in each inner array are indices of neurons within the respective Layer from layers.\n\nArguments\n\nel::WaspnetElement: The element to prune neurons from, either a Network or Layer\nlayers: A list of indices for which Layers we're removing neurons from the Network where it resides\nneurons: A list of lists of neurons to remove in the respective entries from layers.\nl_idx: If prune is called on a Layer, l_idx denotes the index of the that Layer if it were to appear in the list layers\n\n\n\n\n\n","category":"method"},{"location":"reference/#","page":"Reference","title":"Reference","text":"<!– \"src/utilities/pruning.jl\" –>","category":"page"},{"location":"#WaspNet.jl-1","page":"Home","title":"WaspNet.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"WaspNet.jl is a spiking neural network simulator for Julia. ","category":"page"},{"location":"example/#Example-1","page":"Example","title":"Example","text":"","category":"section"},{"location":"example/#","page":"Example","title":"Example","text":"Here we step through an example to demonstrate typical usage of WaspNet. We begin by building a new LIF Neuron type which encapsulates the update rules for a neuron. Then, we make two Layers of neurons to communicate back and forth; one Layer is strictly feed-forward, the other is a recurrent layer accepting inputs both from itself and from the preceding Layer. Finally, we build a Network out of these Layers and simulate it to observe the evolution of the Network as a whole.","category":"page"},{"location":"example/#Getting-Started-1","page":"Example","title":"Getting Started","text":"","category":"section"},{"location":"example/#","page":"Example","title":"Example","text":"The following code has been tested in Julia 1.4.2 and executes without errors. Any dependencies will be called out as necessary. ","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"To start, the only necessary dependency will be WaspNet itself, so we start by importing it. Other useful packages will be Random and BlockArrays","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"using WaspNet","category":"page"},{"location":"example/#Constructing-a-New-Neuron-1","page":"Example","title":"Constructing a New Neuron","text":"","category":"section"},{"location":"example/#","page":"Example","title":"Example","text":"The simlpest unit in WaspNet is the Neuron which translates an input signal from preceding neurons into the evolution of an internal state and ultimately spikes which are sent to neurons down the line.","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"A concrete AbstractNeuron needs to cover 3 things:","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"A new struct which is a subtype of AbstractNeuron; optionally mutable\nAn update method to implement the dynamics of the neuron\nA reset method which restores the neuron to its default state.","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"We will implement the Leaky Integrate-&-Fire neuron model here, but a slightly different implementation is available in WaspNet/src/neurons/lif.jl or with WaspNet.LIF. ","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"A concrete AbstractNeuron implementation currently must include two specific fields: state and output. state holds the current state of the neuron in a Vector and output holds the output of the neuron after its last update; for a spiking neuron, update is either a 0 or a 1 to denote whether a spike did or did not occur. Additional fields should be implemented as needed to parameterize the neuron. ","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"struct LIF{T<:Number}<:AbstractNeuron \n    τ::T\n    R::T\n    θ::T\n    I::T\n\n    v0::T\n    state::T\n    output::T\nend\n","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"Additionally, we need to define how to evolve our neuron given a time step. This is done by adding a method to WaspNet.update! or WaspNet.update, a function which is global across all WaspnetElements. To update a neuron, we provide the neuron we need to update, the input_update to the neuron, the time duration to evolve dt, and the current global time t. In the LIF case, the input_update is a voltage which must be added to the membrane potential of the neuron resulting from spikes in neurons which feed into the current neuron. reset simply restores the state of the neuron to its some state.","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"We use an Euler update for the time evolution because of its simplicity of implementation.","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"Note that both update and reset are defined within WaspNet; that is, we actually define the methods WaspNet.update and WaspNet.reset. If defined externally, these methods are not visible to other methods from within WaspNet.","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"function WaspNet.update(neuron::LIF, input_update, dt, t)\n    output = 0.\n    \n    state = neuron.state + input_update # If an impulse came in, add it\n\n    # Euler method update\n    state += (dt/neuron.τ) * (-state + neuron.R*neuron.I)\n\n    # Check for thresholding\n    if state >= neuron.θ\n        state = neuron.v0\n        output = 1. # Binary output\n    end\n\n    return (output, LIF(neuron.τ, neuron.R, neuron.θ, neuron.I, neuron.v0, state, output))\nend","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"Now we want to instantiate our LIF neuron, update it a few times to see the state of the neuron change","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"neuronLIF = LIF(8., 10.E2, 30., 40., -55., -55., 0.)\n\nprintln(neuronLIF.state)\n# -55.0\nupdate!(neuronLIF, 0., 0.001, 0)\nprintln(neuronLIF.state)\n# -49.993125\n\nneuronLIF = reset(neuronLIF)\nprintln(neuronLIF.state)\n# -55.0","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"We can also simulate! a neuron, chaining together multiple update calls and returning the outputs (spikes) and optionally the internal state of the neuron as well. The following code simulates our LIF neuron for 250 ms with a 0.1 ms time step. The input to the neuron is a function of one parameter, t, defined by (t) -> 0.4*exp(-4t).","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"LIFsim = simulate!(neuronLIF, (t)->0.4*exp(-4t), 0.0001, 0.250, track_state=true);","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"LIFsim is a SimulationResult instance with three fields: LIFsim.outputs, LIFsim.states, and LIFsim.times. times holds all of the times at which the neuron was simualted, outputs holds the output of the neuron at each time step, and states hold the state of the neuron at each time step.","category":"page"},{"location":"example/#Combining-Neurons-into-a-Layer-1","page":"Example","title":"Combining Neurons into a Layer","text":"","category":"section"},{"location":"example/#","page":"Example","title":"Example","text":"In WaspNet, a collection of neurons is called a Layer or a population. A Layer is homogeneous insofar as all of the Neurons in a given Layer must be of the same type, although their individual parameters may differ. The chief utility of a Layer is to handle the computation of the inputs into its constituent Neurons; which is handled through a multiplication of the input spike vector by a corresponding weight matrix, W.","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"The following code constructs a feed-forward Layer with N LIF neurons inside of it with an incoming weight matrix W to handle 2 inputs. ","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"N = 8;\nneurons = [LIF(8., 10.E2, 30., 40., -55., -55., 0.) for _ in 1:N];\nweights = randn(MersenneTwister(13371485), N,2);\nlayer = Layer(neurons, weights);","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"We can also update! a Layer by driving it with some input as we did for our LIF neuron above. Not that input here is actually an Array{Array{<:Number, 1}, 1} and not just Array{<:Number, 1}. The purpose of this is to handle recurrent or non-feed-forward connections; we will discuss this more in Constructing Networks from Layers.","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"reset!(layer)\nupdate!(layer, [[0.5, 0.8]], 0.001, 0)\nprintln(WaspNet.get_neuron_states(layer))\n# [-49.541978928637135, -49.60578871324857, ..., -50.84036022383181]","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"And we can simulate! with the same syntax as before","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"layersim = simulate!(layer, (t) -> [randn(2)], 0.001, 0.25, track_state=true);","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"Now layersim.outputs and layersim.states will be of size NxT where there are N neurons in the Layer and T time steps.","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"There are several Layer constructors and update! methods; for more information, see Reference or type ?Layer or ?update! in the REPL.","category":"page"},{"location":"example/#Constructing-Networks-from-Layers-1","page":"Example","title":"Constructing Networks from Layers","text":"","category":"section"},{"location":"example/#","page":"Example","title":"Example","text":"Once we have Layers available, we need a way to communicate spikes between them. The Network solves exactly that problem: it orchestrates communication of spiking (or output signals in general) between Layers, routing the appropriate outputs between Layers. ","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"We'll start by constructing a new first Layer for our Network similar to how we did before with the added parameter of Nin, the number of inputs we're feeding into the first Layer of the Network.","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"Nin = 2\nN1 = 3\nneurons1 = [LIF(8., 10.E2, 30., 40., -55., -55., 0.) for _ in 1:N1]\nweights1 = randn(MersenneTwister(13371485), N1, Nin)\nlayer1 = Layer(neurons1, weights1);","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"Now we'll make our second Layer. This Layer is special: it will take feed-forward inputs from the first Layer, but also a recurrent connection to itself. This means that we need to specify W slightly differently, and we also need to supply a new field, conns. To handle non-feed-forward connections in a K-layer, W must be declared as a 1x(K+1) BlockArray. ","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"For our case, K=2. Thus, the first block in W holds the input weights corresponding to the Network input, the second block holds the weights for the first Layer, and the third block holds the weights for the second Layer feeding back into itself. Similarly we must supply conns, an array stating which Layers the current Layer connects to. Entries in conns are indexed such that 0 corresponds to the Network input, 1 corresponds to the output of the first Layer and so on. ","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"N2 = 4;\nneurons2 = [LIF(8., 10.E2, 30., 40., -55., -55., 0.) for _ in 1:N2]\n\nW12 = randn(N2, N1) # connections from layer 1\nW22 = 5*randn(N2, N2) # Recurrent connections\nweights2 = [W12, W22]\n\nconns = [1, 2]\n\nlayer2 = Layer(neurons2, weights2, conns);","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"To form a Network, we specify the constituent Layers","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"mynet = Network([layer1, layer2], Nin)","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"update!, reset! work just as they did for Layers and Neurons","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"reset!(mynet)\nupdate!(mynet, 0.4*ones(Nin), 0.001, 0)\nprintln(WaspNet.get_neuron_states(mynet))\n# [-49.61444931320574, -50.42051080817629, ..., -49.993125]","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"As does simulate!","category":"page"},{"location":"example/#","page":"Example","title":"Example","text":"reset!(mynet)\nnetsim = simulate!(mynet, (t) -> 0.4*ones(Nin), 0.001, 1, track_state=true)","category":"page"}]
}
