<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Example · WaspNet</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">WaspNet</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Example</a><ul class="internal"><li><a class="tocitem" href="#Getting-Started-1"><span>Getting Started</span></a></li><li><a class="tocitem" href="#Constructing-a-New-Neuron-1"><span>Constructing a New Neuron</span></a></li><li><a class="tocitem" href="#Combining-Neurons-into-a-Layer-1"><span>Combining Neurons into a Layer</span></a></li><li><a class="tocitem" href="#Constructing-Networks-from-Layers-1"><span>Constructing Networks from Layers</span></a></li></ul></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Example</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Example</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com//blob/master/docs/src/example.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Example-1"><a class="docs-heading-anchor" href="#Example-1">Example</a><a class="docs-heading-anchor-permalink" href="#Example-1" title="Permalink"></a></h1><p>Here we step through an example to demonstrate typical usage of <code>WaspNet</code>. We begin by building a new <code>LIF Neuron</code> type which encapsulates the update rules for a neuron. Then, we make two <code>Layer</code>s of neurons to communicate back and forth; one <code>Layer</code> is strictly feed-forward, the other is a recurrent layer accepting inputs both from itself and from the preceding <code>Layer</code>. Finally, we build a <code>Network</code> out of these <code>Layer</code>s and simulate it to observe the evolution of the <code>Network</code> as a whole.</p><h2 id="Getting-Started-1"><a class="docs-heading-anchor" href="#Getting-Started-1">Getting Started</a><a class="docs-heading-anchor-permalink" href="#Getting-Started-1" title="Permalink"></a></h2><p>The following code has been tested in Julia 1.4.2 and executes without errors. Any dependencies will be called out as necessary. </p><p>To start, the only necessary dependency will be <code>WaspNet</code> itself, so we start by importing it. Other useful packages will be <code>Random</code> and <code>BlockArrays</code></p><pre><code class="language-none">using WaspNet</code></pre><h2 id="Constructing-a-New-Neuron-1"><a class="docs-heading-anchor" href="#Constructing-a-New-Neuron-1">Constructing a New Neuron</a><a class="docs-heading-anchor-permalink" href="#Constructing-a-New-Neuron-1" title="Permalink"></a></h2><p>The simlpest unit in <code>WaspNet</code> is the <code>Neuron</code> which translates an input signal from preceding neurons into the evolution of an internal state and ultimately spikes which are sent to neurons down the line.</p><p>A concrete <code>AbstractNeuron</code> needs to cover 3 things:</p><ul><li>A new <code>struct</code> which is a subtype of <code>AbstractNeuron</code>; optionally mutable</li><li>An <code>update</code> method to implement the dynamics of the neuron</li><li>A <code>reset</code> method which restores the neuron to its default state.</li></ul><p>We will implement the <a href="https://en.wikipedia.org/wiki/Biological_neuron_model#Leaky_integrate-and-fire">Leaky Integrate-&amp;-Fire</a> neuron model here, but a slightly different implementation is available in <code>WaspNet/src/neurons/lif.jl</code> or with <code>WaspNet.LIF</code>. </p><p>A concrete <code>AbstractNeuron</code> implementation currently must include two specific fields: <code>state</code> and <code>output</code>. <code>state</code> holds the current state of the neuron in a <code>Vector</code> and <code>output</code> holds the output of the neuron after its last update; for a spiking neuron, update is either a <code>0</code> or a <code>1</code> to denote whether a spike did or did not occur. Additional fields should be implemented as needed to parameterize the neuron. </p><pre><code class="language-none">struct LIF{T&lt;:Number}&lt;:AbstractNeuron 
    τ::T
    R::T
    θ::T
    I::T

    v0::T
    state::T
    output::T
end
</code></pre><p>Additionally, we need to define how to evolve our neuron given a time step. This is done by adding a method to <code>WaspNet.update!</code> or <code>WaspNet.update</code>, a function which is global across all <code>WaspnetElements</code>. To <code>update</code> a neuron, we provide the <code>neuron</code> we need to update, the <code>input_update</code> to the neuron, the time duration to evolve <code>dt</code>, and the current global time <code>t</code>. In the LIF case, the <code>input_update</code> is a voltage which must be added to the membrane potential of the neuron resulting from spikes in neurons which feed into the current neuron. <code>reset</code> simply restores the state of the neuron to its some state.</p><p>We use an <a href="https://en.wikipedia.org/wiki/Euler_method">Euler update</a> for the time evolution because of its simplicity of implementation.</p><p>Note that both <code>update</code> and <code>reset</code> are defined <em>within</em> <code>WaspNet</code>; that is, we actually define the methods <code>WaspNet.update</code> and <code>WaspNet.reset</code>. If defined externally, these methods are not visible to other methods from within <code>WaspNet</code>.</p><pre><code class="language-none">function WaspNet.update(neuron::LIF, input_update, dt, t)
    output = 0.
    
    state = neuron.state + input_update # If an impulse came in, add it

    # Euler method update
    state += (dt/neuron.τ) * (-state + neuron.R*neuron.I)

    # Check for thresholding
    if state &gt;= neuron.θ
        state = neuron.v0
        output = 1. # Binary output
    end

    return (output, LIF(neuron.τ, neuron.R, neuron.θ, neuron.I, neuron.v0, state, output))
end</code></pre><p>Now we want to instantiate our <code>LIF</code> neuron, update it a few times to see the state of the neuron change</p><pre><code class="language-none">neuronLIF = LIF(8., 10.E2, 30., 40., -55., -55., 0.)

println(neuronLIF.state)
# -55.0
update!(neuronLIF, 0., 0.001, 0)
println(neuronLIF.state)
# -49.993125

neuronLIF = reset(neuronLIF)
println(neuronLIF.state)
# -55.0</code></pre><p>We can also <code>simulate!</code> a neuron, chaining together multiple <code>update</code> calls and returning the outputs (spikes) and optionally the internal state of the neuron as well. The following code simulates our <code>LIF</code> neuron for 250 ms with a 0.1 ms time step. The input to the neuron is a function of one parameter, <code>t</code>, defined by <code>(t) -&gt; 0.4*exp(-4t)</code>.</p><pre><code class="language-none">LIFsim = simulate!(neuronLIF, (t)-&gt;0.4*exp(-4t), 0.0001, 0.250, track_state=true);</code></pre><p><code>LIFsim</code> is a <code>SimulationResult</code> instance with three fields: <code>LIFsim.outputs</code>, <code>LIFsim.states</code>, and <code>LIFsim.times</code>. <code>times</code> holds all of the times at which the neuron was simualted, <code>outputs</code> holds the output of the neuron at each time step, and <code>states</code> hold the state of the neuron at each time step.</p><h2 id="Combining-Neurons-into-a-Layer-1"><a class="docs-heading-anchor" href="#Combining-Neurons-into-a-Layer-1">Combining Neurons into a Layer</a><a class="docs-heading-anchor-permalink" href="#Combining-Neurons-into-a-Layer-1" title="Permalink"></a></h2><p>In <code>WaspNet</code>, a collection of neurons is called a <code>Layer</code> or a population. A <code>Layer</code> is homogeneous insofar as all of the <code>Neuron</code>s in a given <code>Layer</code> must be of the same type, although their individual parameters may differ. The chief utility of a <code>Layer</code> is to handle the computation of the inputs into its constituent <code>Neuron</code>s; which is handled through a multiplication of the input spike vector by a corresponding weight matrix, <code>W</code>.</p><p>The following code constructs a feed-forward <code>Layer</code> with <code>N</code> <code>LIF</code> neurons inside of it with an incoming weight matrix <code>W</code> to handle 2 inputs. </p><pre><code class="language-none">N = 8;
neurons = [LIF(8., 10.E2, 30., 40., -55., -55., 0.) for _ in 1:N];
weights = randn(MersenneTwister(13371485), N,2);
layer = Layer(neurons, weights);</code></pre><p>We can also <code>update!</code> a <code>Layer</code> by driving it with some input as we did for our <code>LIF</code> neuron above. Not that <code>input</code> here is actually an <code>Array{Array{&lt;:Number, 1}, 1}</code> and not just <code>Array{&lt;:Number, 1}</code>. The purpose of this is to handle recurrent or non-feed-forward connections; we will discuss this more in <a href="#Constructing-Networks-from-Layers-1">Constructing Networks from Layers</a>.</p><pre><code class="language-none">reset!(layer)
update!(layer, [[0.5, 0.8]], 0.001, 0)
println(WaspNet.get_neuron_states(layer))
# [-49.541978928637135, -49.60578871324857, ..., -50.84036022383181]</code></pre><p>And we can <code>simulate!</code> with the same syntax as before</p><pre><code class="language-none">layersim = simulate!(layer, (t) -&gt; [randn(2)], 0.001, 0.25, track_state=true);</code></pre><p>Now <code>layersim.outputs</code> and <code>layersim.states</code> will be of size <code>NxT</code> where there are <code>N</code> neurons in the <code>Layer</code> and <code>T</code> time steps.</p><p>There are several <code>Layer</code> constructors and <code>update!</code> methods; for more information, see <a href="../reference/#Reference-1">Reference</a> or type <code>?Layer</code> or <code>?update!</code> in the REPL.</p><h2 id="Constructing-Networks-from-Layers-1"><a class="docs-heading-anchor" href="#Constructing-Networks-from-Layers-1">Constructing Networks from Layers</a><a class="docs-heading-anchor-permalink" href="#Constructing-Networks-from-Layers-1" title="Permalink"></a></h2><p>Once we have <code>Layer</code>s available, we need a way to communicate spikes between them. The <code>Network</code> solves exactly that problem: it orchestrates communication of spiking (or output signals in general) between <code>Layer</code>s, routing the appropriate outputs between <code>Layer</code>s. </p><p>We&#39;ll start by constructing a new first <code>Layer</code> for our <code>Network</code> similar to how we did before with the added parameter of <code>Nin</code>, the number of inputs we&#39;re feeding into the first <code>Layer</code> of the <code>Network</code>.</p><pre><code class="language-none">Nin = 2
N1 = 3
neurons1 = [LIF(8., 10.E2, 30., 40., -55., -55., 0.) for _ in 1:N1]
weights1 = randn(MersenneTwister(13371485), N1, Nin)
layer1 = Layer(neurons1, weights1);</code></pre><p>Now we&#39;ll make our second <code>Layer</code>. This <code>Layer</code> is special: it will take feed-forward inputs from the first <code>Layer</code>, but also a recurrent connection to itself. This means that we need to specify <code>W</code> slightly differently, and we also need to supply a new field, <code>conns</code>. To handle non-feed-forward connections in a <code>K</code>-layer, <code>W</code> must be declared as a <code>1x(K+1)</code> <code>BlockArray</code>. </p><p>For our case, <code>K=2</code>. Thus, the first block in <code>W</code> holds the input weights corresponding to the <code>Network</code> input, the second block holds the weights for the first <code>Layer</code>, and the third block holds the weights for the second <code>Layer</code> feeding back into itself. Similarly we must supply <code>conns</code>, an array stating which <code>Layer</code>s the current <code>Layer</code> connects to. Entries in <code>conns</code> are indexed such that <code>0</code> corresponds to the <code>Network</code> input, <code>1</code> corresponds to the output of the first <code>Layer</code> and so on. </p><pre><code class="language-none">N2 = 4;
neurons2 = [LIF(8., 10.E2, 30., 40., -55., -55., 0.) for _ in 1:N2]

W12 = randn(N2, N1) # connections from layer 1
W22 = 5*randn(N2, N2) # Recurrent connections
weights2 = [W12, W22]

conns = [1, 2]

layer2 = Layer(neurons2, weights2, conns);</code></pre><p>To form a <code>Network</code>, we specify the constituent <code>Layer</code>s</p><pre><code class="language-none">mynet = Network([layer1, layer2], Nin)</code></pre><p><code>update!</code>, <code>reset!</code> work just as they did for <code>Layer</code>s and <code>Neuron</code>s</p><pre><code class="language-none">reset!(mynet)
update!(mynet, 0.4*ones(Nin), 0.001, 0)
println(WaspNet.get_neuron_states(mynet))
# [-49.61444931320574, -50.42051080817629, ..., -49.993125]</code></pre><p>As does <code>simulate!</code></p><pre><code class="language-none">reset!(mynet)
netsim = simulate!(mynet, (t) -&gt; 0.4*ones(Nin), 0.001, 1, track_state=true)</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../reference/">Reference »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 30 July 2020 23:33">Thursday 30 July 2020</span>. Using Julia version 1.4.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
